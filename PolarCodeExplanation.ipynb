{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec14d28",
   "metadata": {},
   "source": [
    "# Polar Code\n",
    "Writer: Hosung Joo (zxcqa123@postech.ac.kr)\n",
    "\n",
    "This document briefly describes how to polar-encode and polar-decode.\n",
    "\n",
    "## Encoding: Channel Polarization\n",
    "\n",
    "Consider 2-bit transmitting channel such that:\n",
    "- one bit is transmitted without any process\n",
    "- another bit is transmitted with <b>XOR</b> operation of two inputs of bits.\n",
    "\n",
    "We call these channels are <i>polarized</i> as it transmits a single bit <b>twice</b>. Thus, the twice-transmitted bit is <i>repetively coded</i>, therefore, it is a good channel. Otherwise, it is a bad channel.\n",
    "We may demonstrate such channel using this $G_{2}$ matrix.\n",
    "\n",
    "$$G_{2} = \\begin{bmatrix}1 & 0\\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Consider inputs $x = \\begin{bmatrix}u_{0} & u_{1}\\end{bmatrix}$.\n",
    "\n",
    "Transmitting $y=x G_{2}$ gives the same as we discussed. See the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aefcd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G2:\n",
      "[[1 0]\n",
      " [1 1]]\n",
      "x1G2:\n",
      "[0 0]\n",
      "x2G2:\n",
      "[1 1]\n",
      "x3G2:\n",
      "[1 0]\n",
      "x4G2:\n",
      "[2 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1 = np.array([0, 0])\n",
    "x2 = np.array([0, 1])\n",
    "x3 = np.array([1, 0])\n",
    "x4 = np.array([1, 1])\n",
    "\n",
    "G2 = np.array([[1, 0],[1, 1]])\n",
    "\n",
    "print(\"G2:\")\n",
    "print(G2)\n",
    "\n",
    "print(\"x1G2:\")\n",
    "print(np.matmul(x1,G2))\n",
    "print(\"x2G2:\")\n",
    "print(np.matmul(x2,G2))\n",
    "print(\"x3G2:\")\n",
    "print(np.matmul(x3,G2))\n",
    "print(\"x4G2:\")\n",
    "print(np.matmul(x4,G2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d614c",
   "metadata": {},
   "source": [
    "Meanwhile, we use 1 or 0 (bits) for the data. In actual, XOR operation for $x_{4}G_{2}$ was needed. The XOR operation can be implemented by adding the bits and then the (mod 2) operation. <b><i>The addition in mod 2</i></b> is the addition in Galois field of $GF(2)$.\n",
    "\n",
    "<details>\n",
    "    <summary>See little details of $GF(2)$. Click here.</summary>\n",
    "\n",
    "### Little note on $GF(2)$:\n",
    "\n",
    "- $GF(2) = \\{K,+,\\times\\}$\n",
    "- elements: $K=\\{0,1\\}$\n",
    "- operations: $\\{+,\\times\\}$\n",
    "\n",
    " - actual operation: $+$ is bitwise XOR, $\\times$ is bitwise AND.\n",
    " $$\\begin{equation} \\label{GF2}\n",
    " \\begin{split}\n",
    " 0 + 0 = 0, & ~ 0 + 1 = 1, \\\\\n",
    " 1 + 0 = 1, & ~ 1 + 1 = 0 \\\\\n",
    " & \\\\\n",
    " 0 \\times 0 = 0, & ~ 0 \\times 1 = 0, \\\\\n",
    " 1 \\times 0 = 0, & ~ 1 \\times 1 = 1 \\\\\n",
    " & \\\\\n",
    " Note: for ~ any ~ & x \\in K, \\\\\n",
    " x + x = 0, & ~ thus, ~ -x = x \\\\\n",
    " x + 0 = x~ & \\\\\n",
    " x \\times x = x, & ~ thus, ~ x^{-1} = x \\\\\n",
    " x \\times 1 = x~&\n",
    " \\end{split}\n",
    " \\end{equation}$$\n",
    " - associativity: for any $x, y, z \\in K$, $x+(y+z)=(x+y)+z$, $x\\times(y\\times z)=(x\\times y)\\times z)$\n",
    " - commutativity: for any $x, y \\in K$, $x+y=y+x$, $x\\times y=y\\times x$\n",
    " - identity: $0$ for $+$, $1$ for $\\times$. Note that $0$ or $1$ is $\\in K$\n",
    " - inverse: <i>itself</i>\n",
    " - distributivity: for any $x, y, z \\in K$, $x\\times(y+z) = x\\times y + x\\times z$\n",
    "\n",
    "Simply,\n",
    "$x+y~(mod~2)$ is $x+y$ in $GF(2)$. So we use the 'mod 2' operation.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a3b08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod 2 of x4G2:\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"mod 2 of x4G2:\")\n",
    "print(np.matmul(x4,G2)%2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd77ea",
   "metadata": {},
   "source": [
    "This $G_{2}$ matrix is a one-to-one mapping for Galois field of $GF(2^{2})$. (Of course $G_{2}$ has the inverse.) Channel polarization is a linear-based process, thus, it is faster than the other complicated encoders. See the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e8496a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Y:\n",
      "[[0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0],\n",
    "             [0, 1],\n",
    "             [1, 0],\n",
    "             [1, 1]])\n",
    "\n",
    "Y = np.matmul(X,G2)%2\n",
    "\n",
    "print(\"X:\")\n",
    "print(X)\n",
    "print(\"Y:\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd97aa",
   "metadata": {},
   "source": [
    "### Good Channel, Bad Channel\n",
    "\n",
    "One may notice that the $u_{0}$ is the bad channel, and the $u_{1}$ is the good channel.\n",
    "\n",
    "One may <b>not</b> use $u_{0}$ for the data transmission, but as a redundancy. (<i>i.e.</i> We call this as <i><b>Frozen Bit</b></i>.)\n",
    "\n",
    "If we only use $u_{1}$ for the data transmission, and we leave $u_{0} = 0$ as the frozen bit, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "257cc098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u0:\n",
      "[[0]\n",
      " [0]]\n",
      "u1:\n",
      "[[0]\n",
      " [1]]\n",
      "Y:\n",
      "[[0 0]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "u0 = X[0:2,0].reshape(2,1)\n",
    "u1 = X[0:2,1].reshape(2,1)\n",
    "print(\"u0:\")\n",
    "print(u0)\n",
    "print(\"u1:\")\n",
    "print(u1)\n",
    "\n",
    "print(\"Y:\")\n",
    "print(Y[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8eb42e",
   "metadata": {},
   "source": [
    "We can clearly see that the channel polarization is based on the repetive coding.\n",
    "\n",
    "It gets more complicated when we transmit more than 2 bits. The solution is \"<b>Recursive</b>.\" If we recursively polarize the channels, let's say, twice, then we transmit 4 bits.\n",
    "\n",
    "Polarizing function was solely demonstrated by $G_{2}$. How can we build $G_{4}$? <b>Recursively</b>. But how recursive?\n",
    "\n",
    "Let's say we have $x=\\begin{bmatrix}u_{0} & u_{1} & u_{2} & u_{3}\\end{bmatrix}$. Can we polarize for $\\begin{bmatrix}w_{0} & w_{2}\\end{bmatrix}$ and $\\begin{bmatrix}w_{1} & w_{3}\\end{bmatrix}$ after polarizing $\\{w\\}$ by multiplying $G_{2}$ for $\\begin{bmatrix}u_{0} & u_{1}\\end{bmatrix}$ and $\\begin{bmatrix}u_{2} & u_{3}\\end{bmatrix}$? <b>Yes</b>.\n",
    "\n",
    "Then $u_{0}$ is a bad-bad channel, $u_{1}$ is a good-bad channel, $u_{2}$ is a bad-good channel, $u_{3}$ is a good-good channel.\n",
    "\n",
    "The equation is below:\n",
    "\n",
    "$$\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "x & =\\begin{bmatrix}u_{0} & u_{1} & u_{2} & u_{3}\\end{bmatrix}=\\begin{bmatrix}U_{01} & U_{23}\\end{bmatrix}\\\\\n",
    "w & =\\begin{bmatrix}U_{01}G_{2} & U_{23}G_{2}\\end{bmatrix}=\\begin{bmatrix}w_{0} & w_{1} & w_{2} & w_{3}\\end{bmatrix}\\\\\n",
    "w' & =w\\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1\\end{bmatrix} = \\begin{bmatrix}w'_{0} & w'_{1} & w'_{2} & w'_{3}\\end{bmatrix} = \\begin{bmatrix}W'_{01} & W'_{23}\\end{bmatrix}\\\\\n",
    "y' & = \\begin{bmatrix}W'_{01}G_{2} & W'_{23}G_{2}\\end{bmatrix} = \\begin{bmatrix}y'_{0} & y'_{1} & y'_{2} & y'_{3}\\end{bmatrix} \\\\\n",
    "y & = \\begin{bmatrix}y_{0} & y_{1} & y_{2} & y_{3}\\end{bmatrix} = \\begin{bmatrix}y'_{0} & y'_{2} & y'_{1} & y'_{3}\\end{bmatrix} = y'\\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1\\end{bmatrix}\n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "One may note that $\\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1\\end{bmatrix}$ is a permutation matrix. All of the computations are linearly composed, thus, we may find a final form of this complex computation:\n",
    "\n",
    "$$\\begin{equation} \\label{eq2}\n",
    "\\begin{split}\n",
    "w & = x\\begin{bmatrix}G_{2} & \\mathbb{0} \\\\ \\mathbb{0} & G_{2}\\end{bmatrix} \\\\\n",
    "w' & = w\\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1\\end{bmatrix} \\\\\n",
    "y' & = w'\\begin{bmatrix}G_{2} & \\mathbb{0} \\\\ \\mathbb{0} & G_{2}\\end{bmatrix} \\\\\n",
    "y & = y'\\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1\\end{bmatrix}\n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\\begin{equation} \\label{eq3}\n",
    "\\begin{split}\n",
    "y & = x \\begin{bmatrix}G_{2} & \\mathbb{0} \\\\ \\mathbb{0} & G_{2}\\end{bmatrix} \\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1\\end{bmatrix} \\begin{bmatrix}G_{2} & \\mathbb{0} \\\\ \\mathbb{0} & G_{2}\\end{bmatrix} \\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1\\end{bmatrix} \\\\\n",
    "& = x \\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 1 & 1\\end{bmatrix} \\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1\\end{bmatrix} \\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 1 & 1\\end{bmatrix} \\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1\\end{bmatrix}\n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "Then we can calculate the 4-bit polarization generator matrix as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dee7507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G4:\n",
      "[[1 0 0 0]\n",
      " [1 1 0 0]\n",
      " [1 0 1 0]\n",
      " [1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "G2_i = np.array([[1,0,0,0],\n",
    "                [1,1,0,0],\n",
    "                [0,0,1,0],\n",
    "                [0,0,1,1]])\n",
    "Perm = np.array([[1,0,0,0],\n",
    "                [0,0,1,0],\n",
    "                [0,1,0,0],\n",
    "                [0,0,0,1]])\n",
    "\n",
    "G4 = np.matmul(np.matmul(np.matmul(G2_i,Perm),G2_i),Perm)\n",
    "\n",
    "print(\"G4:\")\n",
    "print(G4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a9c9e0",
   "metadata": {},
   "source": [
    "Note that the $G_{4}$ just looks like:\n",
    "\n",
    "$$G_{4} = \\begin{bmatrix}1 & 0 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 1 & 0 & 1 & 0 \\\\ 1 & 1 & 1 & 1\\end{bmatrix} = \\begin{bmatrix}G_{2} & \\mathbb{0} \\\\ G_{2} & G_{2}\\end{bmatrix} = G_{2} \\begin{bmatrix}\\mathbb{I} & \\mathbb{0} \\\\ \\mathbb{I} & \\mathbb{I}\\end{bmatrix} = G_{2} \\mathbb{G}_{2}$$\n",
    "\n",
    "This matrix operation is called the <b><i>Kronecker product</i></b>. You may notice later that this Kronecker product can be shown in other coding theory papers such as OFDM <i>Hadamard matrix</i> construction, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9bbe38e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kronecker product of G2 and G2:\n",
      "[[1 0 0 0]\n",
      " [1 1 0 0]\n",
      " [1 0 1 0]\n",
      " [1 1 1 1]]\n",
      "Kronecker product of G4 and G2:\n",
      "[[1 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0]\n",
      " [1 1 1 1 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0]\n",
      " [1 1 0 0 1 1 0 0]\n",
      " [1 0 1 0 1 0 1 0]\n",
      " [1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "def kronecker_product(mat1, mat2, dtype='int32'):\n",
    "    # assuming mat1, mat2 as numpy matrix\n",
    "    x1, y1 = mat1.shape\n",
    "    x2, y2 = mat2.shape\n",
    "    res = np.zeros((x1*x2,y1*y2), dtype=dtype)\n",
    "    for i in range(x2):\n",
    "        for j in range(y2):\n",
    "            res[i*x1:(i+1)*x1, j*y1:(j+1)*y1] = mat1*mat2[i,j]\n",
    "    return res\n",
    "\n",
    "G2 = np.array([[1,0],[1,1]])\n",
    "G4_alt = kronecker_product(G2,G2)\n",
    "print('Kronecker product of G2 and G2:')\n",
    "print(G4_alt)\n",
    "\n",
    "G8_alt = kronecker_product(G4_alt,G2)\n",
    "print('Kronecker product of G4 and G2:')\n",
    "print(G8_alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea3d0a9",
   "metadata": {},
   "source": [
    "Constructing $G_{8}$ is now simple as $G_{8} = G_{4} \\mathbb{G}_{2}$, using the Kronecker product. We may use recursive function for this.\n",
    "\n",
    "Now we may implement such polarization coding (<i>i.e. <b>Polar Code</b></i>) perfectly as below. This encoding function was implemented as a distinguished <i>.py</i> file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b4f82ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original stream is: [0 1 1 1 0 0 1 0 1 1 1]\n",
      "*****(Polar Encoding)*****\n",
      "[[0 1 0 1]\n",
      " [1 1 0 0]\n",
      " [0 0 1 1]\n",
      " [0 1 1 0]]\n",
      "code rate: 3/4\n",
      "1 bits were padded in order to encode in 4-length block.\n",
      "Encoded stream is: [0 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "class PolarEncode():\n",
    "    def __init__(self):\n",
    "        # Generator matrix\n",
    "        self.G = []\n",
    "        self.G.append(np.array([[1,0],[1,1]]))\n",
    "        \n",
    "        # flags --- these values are used in PolarDecode class.\n",
    "        self.frozen_pattern = None\n",
    "        self.zero_padding = -1\n",
    "        \n",
    "    def encode(self, input_seq, block_len, frozen_pattern):\n",
    "        # input_seq: 1d np array\n",
    "        # block_len: int, power of 2\n",
    "        # frozen_pattern: 1d np bit array of length $(block_len), '1' responds frozen.\n",
    "        if (not isinstance(block_len, (int))):\n",
    "            raise TypeError(f'PolarEncode.encode(): block_len (= {block_len}) was not an integer.')\n",
    "        if (not isinstance(input_seq, (np.ndarray))):\n",
    "            raise TypeError(f'PolarEncode.encode(): input_seq was not a numpy.ndarray instance.')\n",
    "        if (not isinstance(frozen_pattern, (np.ndarray))):\n",
    "            raise TypeError(f'PolarEncode.encode(): frozen_pattern was not a numpy.ndarray instance.')\n",
    "        \n",
    "        G_N = np.log2(block_len)\n",
    "        enc_map = self._encode_map(G_N)\n",
    "        \n",
    "        blen, _ = enc_map.shape\n",
    "        if (block_len != blen):\n",
    "            raise ValueError(f'PolarEncode.encode(): block_len (= {block_len}) was not power of 2.')\n",
    "        if (len(frozen_pattern) != blen):\n",
    "            raise ValueError(f'PolarEncode.encode(): frozen pattern has length of {len(frozen_pattern)}, \\\n",
    "while it requires to be {blen}.')\n",
    "        self.frozen_pattern = frozen_pattern\n",
    "        \n",
    "        res = self._frozen_split_stream(input_seq)\n",
    "        res = np.matmul(res,enc_map)%2\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def used_frozen_pattern(self):\n",
    "        if (self.frozen_pattern is None):\n",
    "            raise Exception('This codebook was not used.')\n",
    "        blen = len(self.frozen_pattern)\n",
    "        frozen_bits = self.frozen_pattern.sum()\n",
    "        print(f'code rate: {blen-frozen_bits}/{blen}')\n",
    "        return self.frozen_pattern\n",
    "    \n",
    "    def unite_blocks(self, input_blocks, dtype='int64'):\n",
    "        if (not isinstance(input_blocks, (np.ndarray))):\n",
    "            raise TypeError(f'PolarEncode.unite_blocks(): input_block was not a numpy.ndarray instance.')\n",
    "        bnum, blen = input_blocks.shape\n",
    "        res = np.zeros((bnum*blen,), dtype=dtype)\n",
    "        \n",
    "        for i in range(bnum):\n",
    "            for j in range(blen):\n",
    "                res[i*blen:(i+1)*blen]=input_blocks[i,:]\n",
    "        return res\n",
    "    \n",
    "    def _frozen_split_stream(self, input_seq):\n",
    "        N = len(input_seq)\n",
    "        block_len = len(self.frozen_pattern)\n",
    "        block_num = -(-N//(block_len-self.frozen_pattern.sum()))\n",
    "        res = np.zeros((block_num, block_len), dtype='int32')\n",
    "        \n",
    "        k = 0\n",
    "        for i in range(block_num):\n",
    "            for j, v in enumerate(self.frozen_pattern):\n",
    "                if (v==0):\n",
    "                    if (k<N):\n",
    "                        res[i,j] = input_seq[k]\n",
    "                    k+=1\n",
    "        self.zero_padding = k-N\n",
    "        return res\n",
    "    \n",
    "    def _encode_map(self, G_N):\n",
    "        while (len(self.G) < G_N):\n",
    "            self.G.append(self._kronecker_product(self.G[-1],self.G[0]))\n",
    "        return self.G[int(G_N)-1]\n",
    "\n",
    "    def _kronecker_product(self, mat1, mat2, dtype='int32'):\n",
    "        # assuming mat1, mat2 as numpy matrix\n",
    "        x1, y1 = mat1.shape\n",
    "        x2, y2 = mat2.shape\n",
    "        res = np.zeros((x1*x2,y1*y2), dtype=dtype)\n",
    "        for i in range(x2):\n",
    "            for j in range(y2):\n",
    "                res[i*x1:(i+1)*x1, j*y1:(j+1)*y1] = mat1*mat2[i,j]\n",
    "        return res\n",
    "\n",
    "g = PolarEncode()\n",
    "input_sequence = np.array([0,1,1,1,0,0,1,0,1,1,1])\n",
    "block_length = 4\n",
    "frozen_bit_pattern = np.array([1,0,0,0])\n",
    "print(f'Original stream is: {input_sequence}')\n",
    "\n",
    "print('*****(Polar Encoding)*****')\n",
    "ENCODED_BLOCKS = g.encode(input_sequence, block_length, frozen_bit_pattern)\n",
    "print(ENCODED_BLOCKS)\n",
    "fp = g.used_frozen_pattern()\n",
    "print(f'{g.zero_padding} bits were padded in order to encode in {len(g.frozen_pattern)}-length block.')\n",
    "ENCODED_STREAM = PolarEncode.unite_blocks(None, ENCODED_BLOCKS)\n",
    "print(f'Encoded stream is: {ENCODED_STREAM}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26749eb",
   "metadata": {},
   "source": [
    "## Decoding Scenario of Channel Polarization\n",
    "\n",
    "Now we need the decoding function. First we should note that the optimal (fastest) decoding is still on research. Still, there are challenges to utilize polar code fast and compact.\n",
    "\n",
    "Let's start by recalling $G_{2}$ case.\n",
    "\n",
    "If we freeze $u_{0}=0$, we send $\\begin{bmatrix}0 & 0\\end{bmatrix}$ or $\\begin{bmatrix}1 & 1\\end{bmatrix}$.\n",
    "\n",
    "Let's be simple: <b><i>BPSK</i></b> scenario. Thus, we send $\\mathbb{y}$ while $\\mathbb{y}=\\mathbb{x}G$ and we receive $\\mathbb{r}=\\mathbb{y}+\\mathbb{n}$.\n",
    "\n",
    "$$\\begin{equation} \\label{eq_fig_ChannelCoding_BPSK}\n",
    "\\mathbb{x}-PolarCode(G)-\\mathbb{y}-BPSK(AWGN(\\mathbb{n}))-\\mathbb{r}\n",
    "\\end{equation}$$\n",
    "\n",
    "Other communication scenario also can be decoded if we specify the decision boundaries as well. (Let's make the decision boundary $(v_{d}\\ as\\ decision\\_value, \\delta_{0}\\ as\\ 0\\ direction)$ and the AWGN noise spectral density $N_{o}$ to be input variables of the decoding function.)\n",
    "\n",
    "### BPSK, repetive coding, hard decision scenario\n",
    "\n",
    "If the BPSK sends 0 as $+1$ and 1 as $-1$, $(v_{d}, \\delta_{0}) = (0.0, +1.0)$. ($\\delta_{1} = -1.0$)\n",
    "\n",
    "We can make the decision <i>hard</i> if we decide the received symbol $s$ to be 0 when it satisfies $\\frac{s-v_{d}}{\\delta_{0}-v_{d}}>0$.\n",
    "\n",
    "For example of this BPSK, we have $v_{d}=0$ and $\\delta_{0}=1$, then any received $s$ should be determined as 0 if $\\frac{s-0}{1-0}=s>0$.\n",
    "\n",
    "However, we cannot decide whether <b><i>the repeatedly transmitted bit</i></b> $u_{1}$ was 0 or 1 by this hard decision scenario, if we received $\\mathbb{r}=\\begin{bmatrix}1.5 & -0.1\\end{bmatrix}$, while it is <b><i>more likely</i></b> to be $\\hat{u}_1=0$.\n",
    "\n",
    "Receiving these values (<u>one positive, while the other one negative</u>) happens <b>often enough</b>. You can actually calculate the probability which would depend on the noise variance, and it will definitely not be zero.\n",
    "\n",
    "<b>Summary</b>: Hard decision cannot decide $\\mathbb{r}=\\begin{bmatrix}1.5 & -0.1\\end{bmatrix}$ case.\n",
    "\n",
    "### BPSK, repetive coding, soft decision scenario\n",
    "\n",
    "We want to see what the <b><i>likelihood</i></b> is, thus, we see the <i>likelihood ratio (LR)</i>. The likelihood ratio on transmitting the bit stream $\\mathbb{x}=\\left\\{x_{i}\\right\\}$ and received values $\\mathbb{r}=\\left\\{r_{i}\\right\\}$ is defined by:\n",
    "\n",
    "$$\\begin{equation} \\label{eq4}\n",
    "LR_{i} = \\frac{\\Pr \\left[ {r_{i}|x_{i}=0} \\right]}{\\Pr \\left[ {r_{i}|x_{i}=1} \\right]}\n",
    "\\end{equation}$$\n",
    "\n",
    "We can equivalently hard-decide that $\\hat{u}_{i}=0$ if $LR_{i}>1$, otherwise $\\hat{u}_{i}=1$.\n",
    "\n",
    "If the channel was AWGN, i.e., $r_{i} = \\delta_{i} + n$ where $n \\sim \\mathcal{N}(mean=0,var=N_{symbol}=\\sigma^{2})$, \\\n",
    "the PDF of $n$ follows $p_{n}(r) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\left(\\frac{r-\\delta}{\\sigma}\\right)^{2}}$ where $\\delta$ was actually sent (after processing channel equalization).\n",
    "\n",
    "Then, with known value (again, worth mentioning, by channel equalization from CSI,) $\\delta_{0}$ and $\\delta_{1}$:\n",
    "\n",
    "$\\Pr \\left[ {r_{i}|x_{i}=0} \\right] = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\left(\\frac{r_{i}-\\delta_{0}}{\\sigma}\\right)^{2}}$, $\\Pr \\left[ {r_{i}|x_{i}=1} \\right] = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\left(\\frac{r_{i}-\\delta_{1}}{\\sigma}\\right)^{2}}$.\n",
    "\n",
    "Thus, their ratio cancels out the same constant in AWGN scenario.\n",
    "\n",
    "$$\\begin{equation} \\label{eq5}\n",
    "LR_{i} = \\frac{\\Pr \\left[ {r_{i}|x_{i}=0} \\right]}{\\Pr \\left[ {r_{i}|x_{i}=1} \\right]}\n",
    "\\end{equation} = \\frac{e^{-\\left(\\frac{r_{i}-\\delta_{0}}{\\sigma}\\right)^{2}}}{e^{-\\left(\\frac{r_{i}-\\delta_{1}}{\\sigma}\\right)^{2}}} = e^{-\\left[ \\left(\\frac{r_{i}-\\delta_{0}}{\\sigma}\\right)^{2}-\\left(\\frac{r_{i}-\\delta_{1}}{\\sigma}\\right)^{2} \\right]}$$\n",
    "\n",
    "- (Note that this operation was only based on the <b>likelihood</b>, not <i>a posteriori</i>.)\n",
    "<details>\n",
    "    <summary> - if you are to disduss maximizing a posteriori estimation, <i>click here.</i> </summary>\n",
    "\n",
    "    ### MAPE\n",
    "\n",
    "    Use the Bayes rule:\n",
    "    $$\\begin{equation}\n",
    "    \\Pr \\left[ {x_{i}=0|r_{i}} \\right] = \\frac{\\Pr \\left[ {x_{i}=0} \\right] \\Pr \\left[ {r_{i}|x_{i}=0} \\right]}{\\Pr \\left[ {r_{i}} \\right]}\n",
    "    \\end{equation}$$\n",
    "    \n",
    "</details>\n",
    "\n",
    "The exponent is called <b><i>Log of the Likelihood Ratio (LLR)</i></b>, because it is. It is its log.\n",
    "\n",
    "One may notice that <b>LLR</b> calculation is actually easier <i>(faster!)</i> than <i>LR</i> calculation. We may apply the exponential function later, but we may also use the same hard decision as $\\hat{u}_{i}=0$ if $LLR_{i}>0$, otherwise $\\hat{u}_{i}=1$.\n",
    "\n",
    "We only need $N_{symbol}=\\sigma^2$ and $\\delta_{0}$ with $\\delta_{1}$ as preliminary information, then we can instantly compute $LLR_{i}$ with respect to the received value $r_{i}$ by using the formula:\n",
    "\n",
    "$$\\begin{equation} \\label{eq6}\n",
    "LLR_{i} = \\left(\\frac{r_{i}-\\delta_{1}}{\\sigma}\\right)^{2}-\\left(\\frac{r_{i}-\\delta_{0}}{\\sigma}\\right)^{2}\n",
    " = \\frac{{\\left( {r_{i}-\\delta_{1}} \\right)}^{2} - {\\left( {r_{i}-\\delta_{0}} \\right)}^{2}}{N_{symbol}}\n",
    "\\end{equation}$$\n",
    "\n",
    "We are not done yet. We must calculate how to recover $\\hat{u}_{1}$ when transmitting $\\begin{bmatrix}0 & 0\\end{bmatrix}$ or $\\begin{bmatrix}1 & 1\\end{bmatrix}$. Fortunately, we have only two cases, so we can use the similar LR definition as:\n",
    "\n",
    "$$\\begin{equation} \\label{eq7}\n",
    "LR(\\hat{u}_{1}) = \\frac{\\Pr \\left[ {\\mathbb{r}|\\mathbb{x}=\\begin{bmatrix}0 & 0\\end{bmatrix}} \\right]}{\\Pr \\left[ {\\mathbb{r}|\\mathbb{x}=\\begin{bmatrix}1 & 1\\end{bmatrix}} \\right]}\n",
    "\\end{equation}$$\n",
    "\n",
    "Note that $\\Pr \\left[ {\\mathbb{r}|\\mathbb{x}=\\begin{bmatrix}0 & 0\\end{bmatrix}} \\right]$ is the product of independent random variables. Every noise per symbol (received value) was independent. We just transmitted the same value twice.\n",
    "\n",
    "Thus, $LR(\\hat{u}_{1}) = LR_{0} LR_{1}$. Thus, $LLR(\\hat{u}_{1}) = LLR_{0} + LLR_{1}$ in this case.\n",
    "\n",
    "For example, if we received $\\mathbb{r} = \\begin{bmatrix}1.5 & -0.1\\end{bmatrix}$, then $LLR(\\hat{u}_1) = LLR_{0} + LLR_{1} = \\frac{2.5^2 - 0.5^2 + 0.9^2 - 1.1^2}{N_{symbol}} > 0$, thus, $\\hat{u}_1 = 0$.\n",
    "\n",
    "It gets more complicated when you actually don't know what $u_{0}$ was.\n",
    "\n",
    "### Polarized channel soft decoding\n",
    "\n",
    "What if $x = \\begin{bmatrix}u_{0} & u_{1}\\end{bmatrix}$ and there was no 100% probability of the distribution of $u_{0}$? We start from $\\mathbb{r}$.\n",
    "\n",
    "We clearly see that if $\\delta_{0}=+1$ and $\\delta_{1}=-1$, and $x_{i} \\in \\{\\delta_{0}, \\delta_{1}\\}$,\n",
    "\n",
    "$$\\begin{equation} \\label{eq8}\n",
    "\\begin{split}\n",
    "r_{0} & = y_{0} + n_{0} = x_{0} x_{1} + n_{0} \\\\\n",
    "r_{1} & = y_{1} + n_{0} = x_{1} + n_{1}\n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "We can decide $L(y_{i})$ easily. We should decide $x_{i}$, thus, we should connect $L(y_{i})$ to $L(x_{i})$. \\\n",
    "Note that $\\Pr \\left[ {x} \\right] = \\Pr \\left[ {x|y} \\right] \\Pr \\left[ {y} \\right]$. See the equation below.\n",
    "\n",
    "$$\\begin{equation} \\label{eq9}\n",
    "\\begin{split}\n",
    "\\Pr \\left[ {\\mathbb{y} = \\begin{bmatrix}y_{0} & y_{1}\\end{bmatrix}} \\right] = &\n",
    "\\Pr \\left[ {y_{0}} \\right] \\Pr \\left[ {y_{1}} \\right]\n",
    "\\\\\n",
    "\\Pr \\left[ {u_{0}=0} \\right] = & \\sum_{y_{i} \\in \\{0,1\\}}^{}\n",
    "\\Pr \\left[ {u_{0}=0 | \\mathbb{y} = \\begin{bmatrix}y_{0} & y_{1}\\end{bmatrix}} \\right]\n",
    "\\Pr \\left[ {\\mathbb{y} = \\begin{bmatrix}y_{0} & y_{1}\\end{bmatrix}} \\right] \\\\\n",
    "\\\\\n",
    "= &\n",
    "\\Pr \\left[ {y_{0}=0} \\right] \\Pr \\left[ {y_{1}=0} \\right] +\n",
    "\\Pr \\left[ {y_{0}=1} \\right] \\Pr \\left[ {y_{1}=1} \\right]\n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "Thus, LR of $x_{0}$ is:\n",
    "\n",
    "$$\\begin{equation} \\label{eq10}\n",
    "\\begin{split}\n",
    "LR(u_{0}) = & \\frac{\\Pr \\left[ {u_{0}=0} \\right]}{\\Pr \\left[ {u_{0}=1} \\right]} \\\\\n",
    "= &\n",
    "\\frac{\\Pr \\left[ {y_{0}=0} \\right] \\Pr \\left[ {y_{1}=0} \\right] +\n",
    "\\Pr \\left[ {y_{0}=1} \\right] \\Pr \\left[ {y_{1}=1} \\right]}\n",
    "{\\Pr \\left[ {y_{0}=0} \\right] \\Pr \\left[ {y_{1}=1} \\right] +\n",
    "\\Pr \\left[ {y_{0}=1} \\right] \\Pr \\left[ {y_{1}=0} \\right]} \\\\\n",
    "= &\n",
    "\\frac{\\frac{\\Pr \\left[ {y_{0}=0} \\right]}{\\Pr \\left[ {y_{0}=1} \\right]}\n",
    "\\frac{\\Pr \\left[ {y_{1}=0} \\right]}{\\Pr \\left[ {y_{1}=1} \\right]} + 1}\n",
    "{\\frac{\\Pr \\left[ {y_{0}=0} \\right]}{\\Pr \\left[ {y_{0}=1} \\right]} +\n",
    "\\frac{\\Pr \\left[ {y_{1}=0} \\right]}{\\Pr \\left[ {y_{1}=1} \\right]}} \\\\\n",
    "= &\n",
    "\\frac{LR(y_{0})LR(y_{1}) + 1}{LR(y_{0}) + LR(y_{1})}\n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "Note that LR(y) will be large ($>> 1$) in most cases. Taking the log, we get\n",
    "\n",
    "$$\\begin{equation} \\label{eq11}\n",
    "\\begin{split}\n",
    "LLR(u_{0}) = & ~ \\log{(LR(y_{0})LR(y_{1})+1)} - \\log{(LR(y_{0}) + LR(y_{1})} \\\\\n",
    "\\simeq & ~ \\log{(LR(y_{0})LR(y_{1}))} - \\log{(LR(y_{0}) + LR(y_{1}))} \\\\\n",
    "\\simeq & ~ \\log{(LR(y_{0}))} + \\log{(LR(y_{1}))} - \\log{\\max(LR(y_{0}),LR(y_{1})} \\\\\n",
    "= & ~ \\log{\\min(LR(y_{0}),LR(y_{1})} \\\\\n",
    "= & ~ \\min(LLR(y_{0}),LLR(y_{1})\n",
    "\\end{split}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7480e116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8084a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2790467535213404570404927662635383611165803565110792064097327197730873101389978631706400839291506734463507728257602782531228398600872257773125764593531"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.getrandbits(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a5a08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
